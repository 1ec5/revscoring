{
 "metadata": {
  "name": "",
  "signature": "sha256:e880a00920b514167c112ce6e20b4e77e7c8f303ae0890e31cc57ef5454912a9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# LinearSVC Scorer demonstration\n",
      "\n",
      "In this notebook, we'll be examining the use of a LinearSVC scorer.  First, we'll train and test a machine learning model.  Then we'll construct a scorer using that model and generate some scores.  Then we'll serialize the model into a file for re-use.  \n",
      "\n",
      "Before we get too far, I'm going to import \"pprint\" (Pretty Print) to make it a little bit easier to read the datastructures we are working with.  I'll also be generating some test data, so I'll need to generate some random noise."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pprint import pprint\n",
      "from random import normalvariate"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 1: Training and testing the model\n",
      "\n",
      "First, we get some features for the classifer.  In this case, I've arbitrarily chosen two features that return floating point values. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from revscores.features import (proportion_of_badwords_added,\n",
      "                                proportion_of_markup_added)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we get the LinearSVC out of the \"scorers\" module."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from revscores.scorers import LinearSVC\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since LinearSVC implements MLScorer, it has a \"MODEL\" class variable that points to a model class that we can construct.  When we construct it, we give it the set of features we plan to use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = LinearSVC.MODEL([proportion_of_badwords_added,\n",
      "                         proportion_of_markup_added])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have a model, we're ready to do some training.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = [((normalvariate(0.05, 0.03), normalvariate(0.001, 0.002)), True) for i in range(5000)] + \\\n",
      "            [((normalvariate(0.001, 0.002), normalvariate(0.05, 0.03)), False) for i in range(5000)]\n",
      "stats = model.train(train_set)\n",
      "pprint(stats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'seconds_elapsed': 5.898674488067627}\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've trained the model, we should test it to make sure that it does a good job of making predictions.  While I'm using fake data here, this test phase should really be done with a sample of data that was withheld from training. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_set = [\n",
      "    ([.052, .001], True),\n",
      "    ([.049, .000], True),\n",
      "    ([.073, .000], True),\n",
      "    ([.041, .002], True),\n",
      "    ([.053, .001], False), # This is an anomalous observation and will be mis-predicted\n",
      "    ([.001, .101], False),\n",
      "    ([.000, .107], False),\n",
      "    ([.002, .090], False)\n",
      "]\n",
      "pprint(model.test(test_set))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'auc': 0.8125,\n",
        " 'mean.accuracy': 0.5,\n",
        " 'roc': {'fpr': [0.0, 0.25, 0.25, 0.25, 0.25, 0.5, 0.75, 1.0],\n",
        "         'thresholds': [0.36682442124597303,\n",
        "                        0.35445157617410905,\n",
        "                        0.35388461231087559,\n",
        "                        0.35306794003307962,\n",
        "                        0.34681781324277244,\n",
        "                        0.24906614644646152,\n",
        "                        0.24032728318794708,\n",
        "                        0.23543643557067187],\n",
        "         'tpr': [0.25, 0.25, 0.5, 0.75, 1.0, 1.0, 1.0, 1.0]}}\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 2: Constructing a scorer from the model\n",
      "\n",
      "A scorer's job is to combine a feature extractor with a model so that scores can be requested directly.  So, in order to custruct our scorer, we'll need to build an extractor first.  Since our features use some language features, we'll need to provide a language to the extractor."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mw.api import Session\n",
      "from revscores.extractors import APIExtractor\n",
      "from revscores.language import English\n",
      "\n",
      "extractor = APIExtractor(Session(\"https://en.wikipedia.org/w/api.php\"), language=English())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we have a trained model and an extractor, we can combine them to construct a scorer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scorer = LinearSVC(extractor, model)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use the scorer to score new revisions.  Note that this was trained and tested on data that I made up, so it might not work that well. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pprint(list(scorer.score([639744702, 639746884], probabilities=True)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'prediction': True,\n",
        "  'probabilities': [0.40133802295566051, 0.59866197704433943]},\n",
        " {'prediction': False,\n",
        "  'probabilities': [0.99896021147673797, 0.0010397885232619176]}]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And there we have it."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 3: Storing the model for later\n",
      "\n",
      "Now, for the final part of this demo, we'll serialize the model information into a file so that we can make use of it later.  First, we store the model in a (fake) file using the model's dump() function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from io import BytesIO\n",
      "\n",
      "# Create a file.  We'll use a fake file for this demonstration. \n",
      "f = BytesIO()\n",
      "\n",
      "# Ask the model to dump itself into the file. \n",
      "model.dump(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK.  Now that we have 'f' containing the model.  We can re-read it into a model and rebuild the scorer. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Rewind the BytesIO file to the beginning so that we can read it.\n",
      "f.seek(0)\n",
      "\n",
      "# Use load() on the model class to read the file back in.\n",
      "new_model = LinearSVC.MODEL.load(f)\n",
      "\n",
      "# Rebuild the scorer\n",
      "scorer = LinearSVC(extractor, new_model)\n",
      "\n",
      "# Score some revisions again.\n",
      "pprint(list(scorer.score([639744702, 639746884], probabilities=True)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'prediction': True,\n",
        "  'probabilities': [0.40133802295566051, 0.59866197704433943]},\n",
        " {'prediction': False,\n",
        "  'probabilities': [0.99896021147673797, 0.0010397885232619176]}]\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And there you have it.  We have constructed a MLScorer model, trained it, tested it and stored the whole thing in a file so that we could make use of it later. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}